"""
Base task generator class for Minecraft data collection.

Each task generator corresponds to one task type (chase, orbit, etc.)
and is responsible for:
1. Sampling randomized task configurations
2. Generating docker-compose files with unique ports/displays per worker
3. Running episodes and collecting output
"""

import random
import subprocess
import json
import copy
from datetime import datetime
import tempfile
import re
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, Any, Optional

import yaml

from postprocess.process_recordings import process_bot_recording

from .base_compose_template import get_base_compose_template


PROJECT_ROOT = Path(__file__).resolve().parents[1]


class BaseTaskGenerator(ABC):
    """
    Base class for task generators.
    Each subclass corresponds to one task type (e.g., chase, orbit).
    """
    
    def __init__(
        self,
        task_name: str,
        output_root: Path,
        data_root: Path,
        camera_root: Path,
        base_port: int = 25565,
        base_rcon_port: int = 25575,
        base_vnc_display: int = 99,
        base_vnc_port: int = 5901,
        base_novnc_port: int = 6901,
        worker_id: int = 0,
        seed: Optional[int] = None,
        generate_comparison: bool = False
    ):
        """
        Initialize task generator.
        
        Args:
            task_name: Name of the task type (e.g., "chase", "orbit")
            output_root: Root directory for output files
            data_root: Root directory for MC server data
            camera_root: Root directory for camera data/output
            base_port: Base Minecraft server port
            base_rcon_port: Base RCON port
            base_vnc_display: Base X display number for VNC
            base_vnc_port: Base VNC port
            base_novnc_port: Base noVNC port
            worker_id: Unique worker ID (for port/display assignment)
            seed: Random seed for reproducibility
        """
        self.task_name = task_name
        self.output_root = Path(output_root).absolute()
        self.data_root = Path(data_root).absolute()
        self.camera_root = Path(camera_root).absolute()
        self.project_root = PROJECT_ROOT
        
        self.base_port = base_port
        self.base_rcon_port = base_rcon_port
        self.base_vnc_display = base_vnc_display
        self.base_vnc_port = base_vnc_port
        self.base_novnc_port = base_novnc_port
        self.worker_id = worker_id
        self.generate_comparison = generate_comparison
        
        # Create worker-specific RNG for reproducibility
        self.rng = random.Random(seed if seed is not None else random.randint(0, 2**32-1))
        
        # Counter for episodes generated by this worker
        self.episode_counter = 0
        
        # Create output directories
        self.output_root.mkdir(parents=True, exist_ok=True)
        self.data_root.mkdir(parents=True, exist_ok=True)
        self.camera_root.mkdir(parents=True, exist_ok=True)
    
    def get_unique_id(self) -> str:
        """Generate unique episode ID from task metadata.
        
        Format: {timestamp}_{task}_{worker}_{episode}_{machine}
        Example: 20250121_143052_chase_w000_ep000_hostname_abc123
        """
        import socket
        import hashlib
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        hostname = socket.gethostname()
        machine_hash = hashlib.md5(hostname.encode()).hexdigest()[:6]
        
        return f"{timestamp}_{self.task_name}_w{self.worker_id:03d}_ep{self.episode_counter:06d}_{hostname}_{machine_hash}"
    

    def get_global_env_vars(self) -> Dict[str, str]:
        """Return task-agnostic environment values shared by both bots."""
        return {
            "MC_HOST": "host.docker.internal",
            "RCON_HOST": "host.docker.internal",
            "RCON_PASSWORD": "research",
            "EPISODES_NUM": "1",
            "EPISODE_START_ID": "0",
            "ITERATIONS_NUM_PER_EPISODE": "5",
            "BOOTSTRAP_WAIT_TIME": "20",
            "CAMERA_READY_RETRIES": "30",
            "CAMERA_READY_CHECK_INTERVAL": "2000",
            "TELEPORT_CENTER_X": "0",
            "TELEPORT_CENTER_Z": "0",
            "TELEPORT_RADIUS": "500",
            "WALK_TIMEOUT": "5",
            "MC_VERSION": "1.20.4"
        }
    
    @abstractmethod
    def get_task_env_vars(self) -> Dict[str, str]:
        """
        Return task-specific environment variables as a dictionary.
        Each task type implements its own randomization strategy.
        
        Returns:
            Dictionary of environment variable name -> value (as string)
            
        Example:
            return {
                "CHASE_DURATION_MS": "10000",
                "CHASE_MIN_DISTANCE": "3.0"
            }
        """
        pass
    
    def run_task_episode(self, save_config: bool = True) -> Dict[str, Any]:
        """
        Generate and run a single episode for this task.
        This is the main entry point similar to run_stack.sh.
        
        Args:
            save_config: Whether to save config JSON for reproducibility
            
        Returns:
            Dictionary with episode metadata and results
        """
        # 1. Generate unique episode ID
        unique_id = self.get_unique_id()

        # Compose project names must be lowercase alphanumeric plus -_
        project_slug = re.sub(r"[^a-z0-9_-]", "", unique_id.lower())
        if not project_slug:
            project_slug = "episode"
        project_name = f"mc_{project_slug}"
        
        # 2. Setup unique infrastructure settings based on worker_id
        port_offset = self.worker_id * 100
        mc_port = self.base_port + port_offset
        rcon_port = self.base_rcon_port + port_offset
        vnc_display_alpha = self.base_vnc_display + (self.worker_id * 2)
        vnc_display_bravo = self.base_vnc_display + (self.worker_id * 2) + 1
        vnc_port_alpha = self.base_vnc_port + (self.worker_id * 2)
        vnc_port_bravo = self.base_vnc_port + (self.worker_id * 2) + 1
        novnc_port_alpha = self.base_novnc_port + (self.worker_id * 2)
        novnc_port_bravo = self.base_novnc_port + (self.worker_id * 2) + 1
        receiver_port = 8090
        coord_port_alpha = 8093 + port_offset
        coord_port_bravo = 8094 + port_offset
        instance_id = self.worker_id * 10000 + self.episode_counter
        
        # 3. Create episode-specific output directory
        episode_output_dir = self.output_root / unique_id
        episode_output_dir.mkdir(parents=True, exist_ok=True)
        
        # 4. Setup worker-specific temp directories for MC server data
        worker_data_dir = self.data_root / f"worker_{self.worker_id}"
        episode_data_dir = worker_data_dir / f"episode_{self.episode_counter}"
        data_dir = str(episode_data_dir)
        
        # 5. Camera recordings live under an episode-specific camera prefix
        camera_prefix_dir = episode_output_dir / "camera"
        camera_output_dir_alpha = str(camera_prefix_dir / "output_alpha")
        camera_output_dir_bravo = str(camera_prefix_dir / "output_bravo")
        camera_data_dir_alpha = str(worker_data_dir / "camera_data_alpha")
        camera_data_dir_bravo = str(worker_data_dir / "camera_data_bravo")

        output_dir = str(episode_output_dir)

        # Create all directories
        for dir_path in [data_dir, camera_prefix_dir, camera_data_dir_alpha, camera_data_dir_bravo, 
                         camera_output_dir_alpha, camera_output_dir_bravo]:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
        
        # 6. Sample environment values once so compose + config stay in sync
        global_env_vars = dict(self.get_global_env_vars())
        task_env_vars = dict(self.get_task_env_vars())
        world_seed = self.rng.randint(0, 2**31 - 1)
        bot_rng_seeds = {
            "Alpha": self.rng.randint(0, 2**31 - 1),
            "Bravo": self.rng.randint(0, 2**31 - 1),
        }

        # 7. Generate docker-compose file
        compose_file = self._generate_compose_file(
            episode_id=unique_id,
            mc_port=mc_port,
            rcon_port=rcon_port,
            vnc_display_alpha=vnc_display_alpha,
            vnc_display_bravo=vnc_display_bravo,
            vnc_port_alpha=vnc_port_alpha,
            vnc_port_bravo=vnc_port_bravo,
            novnc_port_alpha=novnc_port_alpha,
            novnc_port_bravo=novnc_port_bravo,
            receiver_port=receiver_port,
            coord_port_alpha=coord_port_alpha,
            coord_port_bravo=coord_port_bravo,
            instance_id=instance_id,
            data_dir=data_dir,
            output_dir=output_dir,
            camera_data_dir_alpha=camera_data_dir_alpha,
            camera_data_dir_bravo=camera_data_dir_bravo,
            camera_output_dir_alpha=camera_output_dir_alpha,
            camera_output_dir_bravo=camera_output_dir_bravo,
            global_env_vars=global_env_vars,
            task_env_vars=task_env_vars,
            world_seed=world_seed,
            bot_rng_seeds=bot_rng_seeds,
        )
        
        # 8. Save episode configuration if requested
        config_data = {
            "unique_id": unique_id,
            "worker_id": self.worker_id,
            "episode_number": self.episode_counter,
            "task_name": self.task_name,
            "timestamp": datetime.now().isoformat(),
            "world_seed": world_seed,
            "bot_rng_seeds": bot_rng_seeds,
            "task_env_vars": task_env_vars,
            "global_env_vars": global_env_vars,
            "compose_project": project_name,
            "ports": {
                "mc_port": mc_port,
                "rcon_port": rcon_port,
                "receiver_port": receiver_port,
                "coord_port_alpha": coord_port_alpha,
                "coord_port_bravo": coord_port_bravo,
                "vnc_port_alpha": vnc_port_alpha,
                "vnc_port_bravo": vnc_port_bravo,
                "novnc_port_alpha": novnc_port_alpha,
                "novnc_port_bravo": novnc_port_bravo,
            },
        }
        if save_config:
            config_file = episode_output_dir / "episode_config.json"
            with open(config_file, 'w') as f:
                json.dump(config_data, f, indent=2)
        
        # 9. Run the stack (similar to run_stack.sh)
        print(f"[Worker {self.worker_id}] Task: {self.task_name}, Episode: {self.episode_counter}")
        print(f"  MC Port: {mc_port}, RCON Port: {rcon_port}")
        print(f"  VNC Displays: :{vnc_display_alpha}, :{vnc_display_bravo}")
        
        start_time = datetime.now()
        try:
            self._cleanup_stale_camera_containers()
            self._run_docker_stack(compose_file, unique_id, project_name, episode_output_dir)
            success = True
            error = None
        except Exception as e:
            success = False
            error = str(e)
            print(f"[Worker {self.worker_id}] Episode {self.episode_counter} failed: {e}")
        finally:
            compose_file.unlink(missing_ok=True)

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # 8. Collect output information
        result = {
            "worker_id": self.worker_id,
            "task_name": self.task_name,
            "episode_number": self.episode_counter,
            "episode_id": unique_id,
            "success": success,
            "error": error,
            "duration_seconds": duration,
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
        }
        
        self.episode_counter += 1
        return result
    
    def _generate_compose_file(
        self,
        episode_id: str,
        mc_port: int,
        rcon_port: int,
        vnc_display_alpha: int,
        vnc_display_bravo: int,
        vnc_port_alpha: int,
        vnc_port_bravo: int,
        novnc_port_alpha: int,
        novnc_port_bravo: int,
        receiver_port: int,
        coord_port_alpha: int,
        coord_port_bravo: int,
        instance_id: int,
        data_dir: str,
        output_dir: str,
        camera_data_dir_alpha: str,
        camera_data_dir_bravo: str,
        camera_output_dir_alpha: str,
        camera_output_dir_bravo: str,
        global_env_vars: Dict[str, str],
        task_env_vars: Dict[str, str],
        world_seed: int,
        bot_rng_seeds: Dict[str, int],
    ) -> Path:
        """
        Generate docker-compose file by overriding base template.
        
        Args:
            episode_id: Unique episode identifier
            mc_port, rcon_port: Server ports
            vnc_display_alpha, vnc_display_bravo: VNC display numbers
            vnc_port_alpha, vnc_port_bravo: VNC ports
            novnc_port_alpha, novnc_port_bravo: noVNC ports
            instance_id: Unique instance ID
            data_dir, output_dir: Directory paths
            camera_data_dir_alpha, camera_data_dir_bravo: Camera data directories
            camera_output_dir_alpha, camera_output_dir_bravo: Camera output directories
            global_env_vars: Shared environment values
            task_env_vars: Task-specific environment overrides
            world_seed: Minecraft world seed
            bot_rng_seeds: Per-bot RNG seed mapping
            
        Returns:
            Path to generated docker-compose file
        """
        compose = copy.deepcopy(get_base_compose_template())
        
        # Get default terrain (flat grass for now, can be overridden by subclasses)
        terrain_json = json.dumps({
            "layers": [
                {"block": "minecraft:bedrock", "height": 1},
                {"block": "minecraft:stone", "height": 124},
                {"block": "minecraft:dirt", "height": 2},
                {"block": "minecraft:grass_block", "height": 1}
            ],
            "biome": "minecraft:plains"
        })
        
        # Override MC server settings
        mc_service = compose["services"]["mc"]
        mc_service["environment"]["SERVER_PORT"] = mc_port
        mc_service["environment"]["RCON_PORT"] = rcon_port
        mc_service["environment"]["SEED"] = str(world_seed)
        mc_service["environment"]["GENERATOR_SETTINGS"] = terrain_json
        mc_service["volumes"] = [f"{data_dir}:/data"]
        mc_service["healthcheck"]["test"] = [
            "CMD-SHELL",
            f"mc-monitor status --host localhost --port {mc_port}"
        ]
        
        # Override sender settings (CRITICAL: Set EPISODE_CATEGORY)
        for bot_name in ["sender_alpha", "sender_bravo"]:
            bot_service = compose["services"][bot_name]

            # Set global environment variables
            bot_service["environment"].update(global_env_vars)

            # Set task-specific environment variables (same for both bots)
            bot_service["environment"].update(task_env_vars)

            # Set bot-specific variables
            if bot_name == "sender_alpha":
                bot_service["environment"]["BOT_NAME"] = "Alpha"
                bot_service["environment"]["OTHER_BOT_NAME"] = "Bravo"
                bot_service["environment"]["COLOR"] = "red"
                bot_service["environment"]["RECEIVER_HOST"] = "receiver_alpha"
                bot_service["environment"]["RECEIVER_PORT"] = str(receiver_port)
                bot_service["environment"]["COORD_PORT"] = str(coord_port_alpha)
                bot_service["environment"]["OTHER_COORD_HOST"] = "sender_bravo"
                bot_service["environment"]["OTHER_COORD_PORT"] = str(coord_port_bravo)
                bot_seed = bot_rng_seeds["Alpha"]
            else:  # sender_bravo
                bot_service["environment"]["BOT_NAME"] = "Bravo"
                bot_service["environment"]["OTHER_BOT_NAME"] = "Alpha"
                bot_service["environment"]["COLOR"] = "blue"
                bot_service["environment"]["RECEIVER_HOST"] = "receiver_bravo"
                bot_service["environment"]["RECEIVER_PORT"] = str(receiver_port)
                bot_service["environment"]["COORD_PORT"] = str(coord_port_bravo)
                bot_service["environment"]["OTHER_COORD_HOST"] = "sender_alpha"
                bot_service["environment"]["OTHER_COORD_PORT"] = str(coord_port_alpha)
                bot_seed = bot_rng_seeds["Bravo"]

            # Set infrastructure variables
            bot_service["environment"]["MC_PORT"] = str(mc_port)
            bot_service["environment"]["RCON_PORT"] = str(rcon_port)
            bot_service["environment"]["BOT_RNG_SEED"] = str(bot_seed)
            bot_service["environment"]["EPISODE_CATEGORY"] = self.task_name

            bot_service["volumes"] = [f"{output_dir}:/output"]

        # Override receiver settings
        for bot_name in ["receiver_alpha", "receiver_bravo"]:
            receiver_service = compose["services"][bot_name]
            receiver_service["environment"]["PORT"] = str(receiver_port)
            receiver_service["environment"]["INSTANCE_ID"] = instance_id
            receiver_service["volumes"] = [f"{output_dir}:/output"]
        
        # Get absolute paths for camera scripts
        camera_scripts_dir = self.project_root / "camera"
        
        # Override camera_alpha settings
        camera_alpha = compose["services"]["camera_alpha"]
        camera_alpha["container_name"] = f"mc_camera_alpha_{episode_id}"
        camera_alpha["environment"]["MC_PORT"] = str(mc_port)
        camera_alpha["environment"]["DISPLAY"] = f":{vnc_display_alpha}"
        camera_alpha["environment"]["VNC_PORT"] = str(vnc_port_alpha)
        camera_alpha["environment"]["NOVNC_PORT"] = str(novnc_port_alpha)
        camera_alpha["volumes"] = [
            f"{camera_data_dir_alpha}:/root",
            f"{camera_output_dir_alpha}:/output",
            f"{camera_scripts_dir}/entrypoint.sh:/app/entrypoint.sh:ro",
            f"{camera_scripts_dir}/launch_minecraft.py:/app/launch_minecraft.py:ro"
        ]
        
        # Override camera_bravo settings
        camera_bravo = compose["services"]["camera_bravo"]
        camera_bravo["container_name"] = f"mc_camera_bravo_{episode_id}"
        camera_bravo["environment"]["MC_PORT"] = str(mc_port)
        camera_bravo["environment"]["DISPLAY"] = f":{vnc_display_bravo}"
        camera_bravo["environment"]["VNC_PORT"] = str(vnc_port_bravo)
        camera_bravo["environment"]["NOVNC_PORT"] = str(novnc_port_bravo)
        camera_bravo["volumes"] = [
            f"{camera_data_dir_bravo}:/root",
            f"{camera_output_dir_bravo}:/output",
            f"{camera_scripts_dir}/entrypoint.sh:/app/entrypoint.sh:ro",
            f"{camera_scripts_dir}/launch_minecraft.py:/app/launch_minecraft.py:ro"
        ]
        
        # Override camera follow helpers
        camera_alpha_follow = compose["services"]["camera_alpha_follow"]
        camera_alpha_follow["container_name"] = f"mc_camera_alpha_follow_{episode_id}"
        camera_alpha_follow["environment"]["RCON_PORT"] = str(rcon_port)
        camera_alpha_follow["volumes"] = [
            f"{camera_scripts_dir}/spectator.js:/app/spectator.js:ro",
            f"{camera_scripts_dir}/package.json:/app/package.json:ro"
        ]
        
        camera_bravo_follow = compose["services"]["camera_bravo_follow"]
        camera_bravo_follow["container_name"] = f"mc_camera_bravo_follow_{episode_id}"
        camera_bravo_follow["environment"]["RCON_PORT"] = str(rcon_port)
        camera_bravo_follow["volumes"] = [
            f"{camera_scripts_dir}/spectator.js:/app/spectator.js:ro",
            f"{camera_scripts_dir}/package.json:/app/package.json:ro"
        ]
        
        # Write to temp file
        temp_dir = Path(tempfile.gettempdir()) / f"mc_worker_{self.worker_id}"
        temp_dir.mkdir(parents=True, exist_ok=True)
        compose_file = temp_dir / f"docker-compose-{episode_id}.yml"
        
        with open(compose_file, 'w') as f:
            yaml.dump(compose, f, default_flow_style=False, sort_keys=False)
        
        return compose_file
    
    def _run_docker_stack(
        self,
        compose_file: Path,
        unique_id: str,
        project_name: str,
        episode_output_dir: Path,
    ):
        """
        Run docker compose stack (mimics run_stack.sh).
        
        Args:
            compose_file: Path to docker-compose file
            unique_id: Unique episode identifier
            project_name: Docker Compose project name (sanitized)
            episode_output_dir: Episode output directory for logs
        """
        log_dir = episode_output_dir / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # Pull images
            print(f"[Worker {self.worker_id}] Pulling images...")
            result = subprocess.run(
                ["docker", "compose", "-p", project_name, "-f", str(compose_file), "pull"],
                capture_output=True,
                text=True,
                cwd=self.project_root
            )
            if result.returncode != 0:
                print(f"[Worker {self.worker_id}] Pull failed:")
                print(result.stderr)
                raise subprocess.CalledProcessError(result.returncode, result.args, result.stdout, result.stderr)
            
            # Start stack
            print(f"[Worker {self.worker_id}] Starting stack...")
            result = subprocess.run(
                ["docker", "compose", "-p", project_name, "-f", str(compose_file), "up", "-d"],
                capture_output=True,
                text=True,
                cwd=self.project_root
            )
            if result.returncode != 0:
                print(f"[Worker {self.worker_id}] Start failed:")
                print(result.stderr)
                raise subprocess.CalledProcessError(result.returncode, result.args, result.stdout, result.stderr)
            
            # Start capturing logs for all services
            log_services = ["mc", "sender_alpha", "sender_bravo", "receiver_alpha", "receiver_bravo",
                          "camera_alpha", "camera_bravo", "camera_alpha_follow", "camera_bravo_follow"]
            log_processes = []
            
            for service in log_services:
                log_file = log_dir / f"{service}.log"
                print(f"[Worker {self.worker_id}] Capturing logs for {service} -> {log_file}")
                with open(log_file, 'w') as f:
                    proc = subprocess.Popen(
                        ["docker", "compose", "-p", project_name, "-f", str(compose_file), 
                         "logs", "--no-color", "--timestamps", "--follow", service],
                        stdout=f,
                        stderr=subprocess.STDOUT,
                        cwd=self.project_root
                    )
                    log_processes.append(proc)
            
            # Wait for senders to complete
            print(f"[Worker {self.worker_id}] Waiting for senders to complete...")
            print(f"[Worker {self.worker_id}] Logs are being saved to: {log_dir}")
            result = subprocess.run(
                ["docker", "compose", "-p", project_name, "-f", str(compose_file),
                 "wait", "sender_alpha", "sender_bravo"],
                timeout=600,  # 10 minute timeout
                capture_output=True,
                text=True,
                cwd=self.project_root
            )
            if result.returncode != 0:
                print(f"[Worker {self.worker_id}] Wait failed:")
                print(result.stderr)
                raise subprocess.CalledProcessError(result.returncode, result.args, result.stdout, result.stderr)
            
            print(f"[Worker {self.worker_id}] Senders completed, shutting down...")
            
            # Stop log capture
            for proc in log_processes:
                proc.terminate()
                try:
                    proc.wait(timeout=2)
                except subprocess.TimeoutExpired:
                    proc.kill()
            
        finally:
            # Always shutdown stack (even if there was an error)
            subprocess.run(
                ["docker", "compose", "-p", project_name, "-f", str(compose_file), "down", "-v"],
                capture_output=True,
                text=True,
                cwd=self.project_root
            )
            
            # Remove camera containers explicitly (they have fixed names and don't get removed by compose down)
            camera_containers = [
                f"mc_camera_alpha_{unique_id}",
                f"mc_camera_bravo_{unique_id}",
                f"mc_camera_alpha_follow_{unique_id}",
                f"mc_camera_bravo_follow_{unique_id}"
            ]
            for container in camera_containers:
                subprocess.run(
                    ["docker", "rm", "-f", container],
                    capture_output=True,
                    text=True
                )
        
        # Run post-processing (align videos + optional comparison)
        print(f"[Worker {self.worker_id}] Running post-processing alignment...")

        camera_prefix = episode_output_dir / "camera"

        def _run_post_for_bot(
            bot: str,
            actions_path: Path,
            camera_meta_path: Path,
            mineflayer_video_path: Path,
            aligned_video_path: Path,
            aligned_meta_path: Path,
            comparison_output_path: Path,
        ) -> None:
            if not actions_path.exists():
                print(f"[Worker {self.worker_id}] {bot}: actions missing at {actions_path}, skipping")
                return
            if not camera_meta_path.exists():
                print(f"[Worker {self.worker_id}] {bot}: camera metadata missing at {camera_meta_path}, skipping")
                return
            if not mineflayer_video_path.exists():
                print(f"[Worker {self.worker_id}] {bot}: mineflayer video missing at {mineflayer_video_path}, skipping")
                return

            try:
                result = process_bot_recording(
                    actions_path=actions_path,
                    camera_meta_path=camera_meta_path,
                    mineflayer_video_path=mineflayer_video_path,
                    output_dir=episode_output_dir,
                    aligned_video_path=aligned_video_path,
                    aligned_metadata_path=aligned_meta_path,
                    comparison_output_path=comparison_output_path,
                    generate_comparison=self.generate_comparison,
                )
                print(
                    f"[Worker {self.worker_id}] {bot}: aligned -> {result['aligned_video_path']}"
                )
                if result["comparison_video_path"]:
                    print(
                        f"[Worker {self.worker_id}] {bot}: comparison -> {result['comparison_video_path']}"
                    )
            except Exception as exc:  # noqa: BLE001 - log and continue
                print(f"[Worker {self.worker_id}] {bot}: post-processing failed: {exc}")

        _run_post_for_bot(
            "Alpha",
            episode_output_dir / "Alpha_mineflayer.json",
            camera_prefix / "output_alpha" / "camera_alpha_meta.json",
            episode_output_dir / "Alpha_mineflayer.mp4",
            episode_output_dir / "camera_alpha_aligned.mp4",
            episode_output_dir / "camera_alpha_aligned_meta.json",
            episode_output_dir / "camera_alpha_comparison.mp4",
        )

        _run_post_for_bot(
            "Bravo",
            episode_output_dir / "Bravo_mineflayer.json",
            camera_prefix / "output_bravo" / "camera_bravo_meta.json",
            episode_output_dir / "Bravo_mineflayer.mp4",
            episode_output_dir / "camera_bravo_aligned.mp4",
            episode_output_dir / "camera_bravo_aligned_meta.json",
            episode_output_dir / "camera_bravo_comparison.mp4",
        )

        print(f"[Worker {self.worker_id}] Episode complete!")
        print(f"[Worker {self.worker_id}] Logs saved to: {log_dir}")

    def _cleanup_stale_camera_containers(self) -> None:
        """Remove any lingering camera containers from previous runs."""
        prefixes = (
            "mc_camera_alpha_",
            "mc_camera_bravo_",
            "mc_camera_alpha_follow_",
            "mc_camera_bravo_follow_",
        )
        containers_to_remove: list[str] = []
        for prefix in prefixes:
            result = subprocess.run(
                [
                    "docker",
                    "ps",
                    "-a",
                    "--filter",
                    f"name={prefix}",
                    "--format",
                    "{{.ID}}",
                ],
                capture_output=True,
                text=True,
            )
            if result.returncode != 0:
                continue
            ids = [line.strip() for line in result.stdout.splitlines() if line.strip()]
            containers_to_remove.extend(ids)

        if not containers_to_remove:
            return

        for container_id in containers_to_remove:
            subprocess.run(
                ["docker", "rm", "-f", container_id],
                capture_output=True,
                text=True,
            )
