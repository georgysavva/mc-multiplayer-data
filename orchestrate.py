#!/usr/bin/env python3
"""
Orchestration script to manage multiple Docker Compose instances for parallel data collection.
"""

import argparse
import os
import subprocess
import sys
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Optional

import signal
import yaml
import shutil


class LogManager:
    def __init__(self, base_dir: Path, compose_bin: Optional[List[str]] = None):
        self.base_dir = base_dir
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.compose_bin = compose_bin or ["docker", "compose"]

    def start_for_instance(
        self,
        compose_file: Path,
        project: str,
        services: List[str],
    ) -> Dict[str, subprocess.Popen]:
        """Start background log capture for the given services.

        Returns a mapping of service -> Popen handle.
        """
        logs_dir = self.base_dir / project
        logs_dir.mkdir(parents=True, exist_ok=True)
        processes: Dict[str, subprocess.Popen] = {}
        for service in services:
            logfile = logs_dir / f"{service}.log"
            # Open file handle in text mode; let Popen own it
            fhandle = open(logfile, "w")
            cmd = [
                *self.compose_bin,
                "-p",
                project,
                "-f",
                str(compose_file),
                "logs",
                "--no-color",
                "--timestamps",
                "--follow",
                service,
            ]
            try:
                proc = subprocess.Popen(cmd, stdout=fhandle, stderr=subprocess.STDOUT)
                processes[service] = proc
            except Exception:
                try:
                    fhandle.close()
                except Exception:  # pragma: no cover
                    pass
        return processes

    def stop_for_instance(self, processes: Dict[str, subprocess.Popen]) -> None:
        for svc, proc in processes.items():
            try:
                if proc.poll() is None:
                    proc.terminate()
                    try:
                        proc.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        proc.kill()
            except Exception:
                continue


def detect_compose_bin() -> List[str]:
    """Detect docker compose binary preference.

    Prefer 'docker compose' if available, otherwise fallback to 'docker-compose'.
    """
    try:
        res = subprocess.run(["docker", "compose", "version"], capture_output=True)
        if res.returncode == 0:
            return ["docker", "compose"]
    except Exception:
        pass
    if shutil.which("docker-compose"):
        return ["docker-compose"]
    # Fallback to docker compose
    return ["docker", "compose"]


class InstanceManager:
    def __init__(self, compose_dir="compose_configs", build_images=False, logs_dir="logs"):
        self.compose_dir = Path(compose_dir)
        self.running_instances = {}
        self.compose_bin = detect_compose_bin()
        self.logs_dir = Path(logs_dir)
        self.log_manager = LogManager(self.logs_dir, self.compose_bin)
        self.build_images = build_images
        # service base names as generated by generate_compose.py
        self.service_bases = [
            "prep_data_instance_{i}",
            "mc_instance_{i}",
            "sender_alpha_instance_{i}",
            "sender_bravo_instance_{i}",
            "receiver_alpha_instance_{i}",
            "receiver_bravo_instance_{i}",
            "camera_alpha_instance_{i}",
            "episode_starter_instance_{i}",
            "camera_bravo_instance_{i}",
            "spectator_alpha_instance_{i}",
            "spectator_bravo_instance_{i}",
        ]
        self.instance_logs: Dict[str, Dict[str, subprocess.Popen]] = {}

    def get_compose_files(self):
        """Get all Docker Compose files in the config directory."""
        return sorted(list(self.compose_dir.glob("docker-compose-*.yml")))

    def build_instance(self, compose_file):
        """Build Docker images for a single instance."""
        instance_name = compose_file.stem
        print(f"Building images for: {instance_name}")

        try:
            cmd = [
                *self.compose_bin,
                "-p",
                instance_name,
                "-f",
                str(compose_file),
                "build",
            ]
            result = subprocess.run(
                cmd, capture_output=True, text=True, cwd=self.compose_dir.parent
            )

            if result.returncode == 0:
                print(f"‚úÖ Built images: {instance_name}")
                return True
            else:
                print(f"‚ùå Failed to build {instance_name}: {result.stderr}")
                return False

        except Exception as e:
            print(f"‚ùå Error building {instance_name}: {e}")
            return False

    def start_instance(self, compose_file):
        """Start a single Docker Compose instance."""
        instance_name = compose_file.stem
        
        # Build images if requested
        if self.build_images:
            if not self.build_instance(compose_file):
                return False, instance_name, None
        
        print(f"Starting instance: {instance_name}")

        try:
            cmd = [
                *self.compose_bin,
                "-p",
                instance_name,
                "-f",
                str(compose_file),
                "up",
                "-d",
            ]
            result = subprocess.run(
                cmd, capture_output=True, text=True, cwd=self.compose_dir.parent
            )

            if result.returncode == 0:
                print(f"‚úÖ Started: {instance_name}")
                # Begin per-service log capture
                idx = self._instance_index_from_stem(instance_name)
                services = [s.format(i=idx) for s in self.service_bases]
                self.instance_logs[instance_name] = self.log_manager.start_for_instance(
                    compose_file, instance_name, services
                )
                # Print VNC/noVNC URLs
                self._print_vnc_urls(compose_file, instance_name)
                return True, instance_name, compose_file
            else:
                print(f"‚ùå Failed to start {instance_name}: {result.stderr}")
                return False, instance_name, None

        except Exception as e:
            print(f"‚ùå Error starting {instance_name}: {e}")
            return False, instance_name, None

    def stop_instance(self, compose_file):
        """Stop a single Docker Compose instance."""
        instance_name = compose_file.stem
        print(f"Stopping instance: {instance_name}")

        try:
            # Stop log capture first
            self.log_manager.stop_for_instance(self.instance_logs.pop(instance_name, {}))
            cmd = [
                *self.compose_bin,
                "-p",
                instance_name,
                "-f",
                str(compose_file),
                "down",
                "-v",
            ]
            result = subprocess.run(
                cmd, capture_output=True, text=True, cwd=self.compose_dir.parent
            )

            if result.returncode == 0:
                print(f"‚úÖ Stopped: {instance_name}")
                return True, instance_name
            else:
                print(f"‚ùå Failed to stop {instance_name}: {result.stderr}")
                return False, instance_name

        except Exception as e:
            print(f"‚ùå Error stopping {instance_name}: {e}")
            return False, instance_name

    def wait_for_senders(self, instance_name, compose_file):
        """Wait for sender services to complete for a single instance."""
        idx = self._instance_index_from_stem(instance_name)
        sender_alpha = f"sender_alpha_instance_{idx}"
        sender_bravo = f"sender_bravo_instance_{idx}"
        
        print(f"[{instance_name}] Waiting for senders to complete...")
        
        try:
            cmd = [
                *self.compose_bin,
                "-p",
                instance_name,
                "-f",
                str(compose_file),
                "wait",
                sender_alpha,
                sender_bravo,
            ]
            result = subprocess.run(
                cmd, capture_output=True, text=True, cwd=self.compose_dir.parent
            )
            
            if result.returncode == 0:
                print(f"[{instance_name}] ‚úÖ Senders completed successfully")
                return True
            else:
                print(f"[{instance_name}] ‚ö†Ô∏è Sender wait failed: {result.stderr}")
                return False
        except Exception as e:
            print(f"[{instance_name}] ‚ùå Error waiting for senders: {e}")
            return False

    def start_all(self):
        """Start all instances in parallel, wait for completion, then stop."""
        compose_files = self.get_compose_files()
        total_instances = len(compose_files)

        if total_instances == 0:
            print("No Docker Compose files found. Run generate_compose.py first.")
            return

        if self.build_images:
            print(f"Building images and starting {total_instances} instances in parallel...")
        else:
            print(f"Pulling images and starting {total_instances} instances in parallel...")

        # Start all instances simultaneously
        started_instances = {}
        with ThreadPoolExecutor(max_workers=total_instances) as executor:
            futures = {
                executor.submit(self.start_instance, cf): cf for cf in compose_files
            }

            for future in as_completed(futures):
                success, instance_name, compose_file = future.result()
                if success:
                    self.running_instances[instance_name] = futures[future]
                    started_instances[instance_name] = compose_file

        print(
            f"\nüéâ Started {len(self.running_instances)}/{total_instances} instances successfully"
        )
        
        # Wait for all sender services to complete
        print(f"\n‚è≥ Waiting for all sender services to complete...")
        
        with ThreadPoolExecutor(max_workers=total_instances) as executor:
            wait_futures = {
                executor.submit(self.wait_for_senders, inst_name, cf): inst_name
                for inst_name, cf in started_instances.items()
            }
            
            completed_count = 0
            for future in as_completed(wait_futures):
                if future.result():
                    completed_count += 1
        
        print(f"\n‚úÖ {completed_count}/{len(started_instances)} instances completed successfully")
        
        # Stop all instances
        print(f"\nüõë Shutting down all instances...")
        self.stop_all()

    def stop_all(self):
        """Stop all instances."""
        compose_files = self.get_compose_files()
        total_instances = len(compose_files)

        if total_instances == 0:
            print("No Docker Compose files found.")
            return

        print(f"Stopping {total_instances} instances...")

        with ThreadPoolExecutor(max_workers=total_instances) as executor:
            futures = {
                executor.submit(self.stop_instance, cf): cf for cf in compose_files
            }

            stopped_count = 0
            for future in as_completed(futures):
                success, instance_name = future.result()
                if success:
                    stopped_count += 1

        print(f"\nüõë Stopped {stopped_count}/{total_instances} instances")

    def status(self):
        """Show status of all instances."""
        compose_files = self.get_compose_files()

        if not compose_files:
            print("No Docker Compose files found.")
            return

        print(f"Found {len(compose_files)} configured instances:")
        print("\nChecking status...")

        running_count = 0
        for compose_file in compose_files:
            instance_name = compose_file.stem
            try:
                cmd = [
                    "docker",
                    "compose",
                    "-p",
                    instance_name,
                    "-f",
                    str(compose_file),
                    "ps",
                    "-q",
                ]
                result = subprocess.run(
                    cmd, capture_output=True, text=True, cwd=self.compose_dir.parent
                )

                if result.stdout.strip():
                    # Check if containers are actually running
                    container_ids = result.stdout.strip().split("\n")
                    container_ids = [
                        cid for cid in container_ids if cid.strip()
                    ]  # Filter empty strings

                    running_containers = 0
                    for container_id in container_ids:
                        inspect_cmd = [
                            "docker",
                            "inspect",
                            "--format",
                            "{{.State.Status}}",
                            container_id,
                        ]
                        inspect_result = subprocess.run(
                            inspect_cmd, capture_output=True, text=True
                        )
                        if inspect_result.stdout.strip() == "running":
                            running_containers += 1

                    if running_containers > 0:
                        print(
                            f"üü¢ {instance_name}: {running_containers} containers running"
                        )
                        running_count += 1
                    else:
                        print(f"üü° {instance_name}: containers exist but not running")
                else:
                    print(f"üî¥ {instance_name}: stopped")

            except Exception as e:
                print(f"‚ùì {instance_name}: error checking status - {e}")

        print(f"\nSummary: {running_count}/{len(compose_files)} instances running")

    def logs(self, instance_pattern=None, follow=False, tail: int = 50):
        """Tail saved log files captured by LogManager.

        If no saved logs exist yet, fallback to docker compose logs.
        """
        logs_root = self.logs_dir
        instances = []
        if instance_pattern:
            for p in logs_root.glob(f"*{instance_pattern}*"):
                if p.is_dir():
                    instances.append(p)
        else:
            instances = [p for p in logs_root.iterdir() if p.is_dir()]

        if not instances:
            # Fallback behavior
            print("No saved logs found; falling back to docker compose logs")
            compose_files = self.get_compose_files()
            if instance_pattern:
                compose_files = [cf for cf in compose_files if instance_pattern in cf.stem]
            for compose_file in compose_files:
                cmd = [
                    *self.compose_bin,
                    "-p",
                    compose_file.stem,
                    "-f",
                    str(compose_file),
                    "logs",
                    "--tail",
                    "50",
                ]
                subprocess.run(cmd, cwd=self.compose_dir.parent)
            return

        for inst_dir in instances:
            print(f"\n{'='*50}")
            print(f"Saved logs for {inst_dir.name}")
            print(f"{'='*50}")
            for logf in sorted(inst_dir.glob("*.log")):
                print(f"-- {logf.name} (last {tail} lines) --")
                try:
                    with open(logf, "r", encoding="utf-8", errors="ignore") as fh:
                        lines = fh.readlines()
                        for line in lines[-tail:]:
                            sys.stdout.write(line)
                except Exception as e:
                    print(f"failed to read {logf}: {e}")

    def _instance_index_from_stem(self, stem: str) -> int:
        # Expect docker-compose-XYZ
        try:
            idx = int(stem.split("-")[-1])
            return idx
        except Exception:
            return 0

    def _print_vnc_urls(self, compose_file: Path, instance_name: str) -> None:
        try:
            with open(compose_file, "r", encoding="utf-8") as fh:
                data = yaml.safe_load(fh)
            # try to locate env NOVNC_PORT for alpha/bravo
            ports = {}
            for svc_name, svc in (data.get("services") or {}).items():
                env = svc.get("environment") or {}
                if svc_name.startswith("camera_alpha_instance_") and "NOVNC_PORT" in env:
                    ports["alpha"] = str(env["NOVNC_PORT"]).strip()
                if svc_name.startswith("camera_bravo_instance_") and "NOVNC_PORT" in env:
                    ports["bravo"] = str(env["NOVNC_PORT"]).strip()
            if ports:
                a = ports.get("alpha")
                b = ports.get("bravo")
                if a:
                    print(f"[orchestrate] {instance_name} noVNC alpha: http://localhost:{a}")
                if b:
                    print(f"[orchestrate] {instance_name} noVNC bravo: http://localhost:{b}")
        except Exception:
            return

    def recordings(self) -> None:
        root = Path.cwd()
        alpha_root = root / "camera" / "output_alpha"
        bravo_root = root / "camera" / "output_bravo"
        print("[orchestrate] camera recordings (alpha):")
        if alpha_root.exists():
            for p in sorted(alpha_root.rglob("camera_*.mp4")):
                print(f"  {p}")
        else:
            print("  (none)")
        print("[orchestrate] camera recordings (bravo):")
        if bravo_root.exists():
            for p in sorted(bravo_root.rglob("camera_*.mp4")):
                print(f"  {p}")
        else:
            print("  (none)")

    def _process_single_recording(self, job, comparison_video, output_dir=None):
        """Process a single episode recording."""
        script_path = Path(__file__).parent / "postprocess" / "process_recordings.py"
        
        cmd = [
            sys.executable,
            str(script_path),
            "--bot", job['bot'],
            "--actions-dir", str(job['output_dir']),
            "--camera-prefix", str(job['camera_prefix']),
            "--episode-file", str(job['episode_file']),
            "--output-dir", output_dir or "/mnt/disks/storage/data/mc_multiplayer_v1/batch1/aligned",
        ]

        if comparison_video:
            cmd.append("--comparison-video")
        
        try:
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=600  # 10 minute timeout per episode
            )
            if result.returncode != 0:
                # Print stderr to help debug
                if result.stderr:
                    print(f"  ERROR: {result.stderr.strip()}")
                if result.stdout:
                    print(f"  OUTPUT: {result.stdout.strip()}")
            return result.returncode == 0
        except subprocess.TimeoutExpired:
            print(f"  Timeout processing {job['episode_file'].name}")
            return False
        except Exception as e:
            print(f"  Error processing {job['episode_file'].name}: {e}")
            return False

    def postprocess_recordings(self, workers=4, comparison_video=False, debug=False, output_dir=None):
        """Process camera recordings for all instances in parallel."""
        print(f"Discovering episodes from orchestrated instances...")
        
        compose_files = self.get_compose_files()
        if not compose_files:
            print("No compose files found.")
            return
        
        # Build list of all episode processing jobs
        jobs = []
        project_root = Path.cwd()
        
        if debug:
            print(f"[DEBUG] Project root: {project_root}")
        
        for compose_file in compose_files:
            instance_id = self._instance_index_from_stem(compose_file.stem)
            
            # Parse compose file to get directories
            with open(compose_file) as f:
                config = yaml.safe_load(f)
            
            # Extract output directory from sender service
            sender_service = config['services'].get(f'sender_alpha_instance_{instance_id}')
            if not sender_service:
                continue

            # Parse volume mount: /host/path:/output (actions output dir)
            actions_output_dir = None
            for vol in sender_service.get('volumes', []):
                if ':/output' in vol:
                    actions_output_dir = Path(vol.split(':')[0])
                    break

            if not actions_output_dir:
                continue

            # Extract camera output directories from camera services
            camera_alpha_service = config['services'].get(f'camera_alpha_instance_{instance_id}')
            camera_bravo_service = config['services'].get(f'camera_bravo_instance_{instance_id}')

            camera_output_alpha = None
            camera_output_bravo = None

            if camera_alpha_service:
                for vol in camera_alpha_service.get('volumes', []):
                    if ':/output' in vol:
                        camera_output_alpha = Path(vol.split(':')[0])
                        break

            if camera_bravo_service:
                for vol in camera_bravo_service.get('volumes', []):
                    if ':/output' in vol:
                        camera_output_bravo = Path(vol.split(':')[0])
                        break
            
            # Find all episode JSON files in actions output directory
            for json_path in sorted(actions_output_dir.glob("*.json")):
                # Skip non-episode files and those not belonging to this instance
                instance_tag = f"_instance_{instance_id:03d}"
                if (
                    json_path.name.endswith("_meta.json")
                    or json_path.name.endswith("_episode_info.json")
                    or instance_tag not in json_path.name
                ):
                    continue
                
                # Determine bot from filename
                if "_Alpha_" in json_path.name:
                    bot = "Alpha"
                    # Use camera output directory from compose file
                    camera_prefix = camera_output_alpha or (project_root / "camera" / "output_alpha" / str(instance_id))
                elif "_Bravo_" in json_path.name:
                    bot = "Bravo"
                    # Use camera output directory from compose file
                    camera_prefix = camera_output_bravo or (project_root / "camera" / "output_bravo" / str(instance_id))
                else:
                    continue
                
                if debug:
                    print(f"[DEBUG] Episode: {json_path.name}")
                    print(f"[DEBUG]   Bot: {bot}, Instance: {instance_id}")
                    print(f"[DEBUG]   Camera prefix: {camera_prefix}")
                    print(f"[DEBUG]   Output dir: {actions_output_dir}")
                
                jobs.append({
                    'episode_file': json_path,
                    'bot': bot,
                    'instance_id': instance_id,
                    'output_dir': actions_output_dir,
                    'camera_prefix': camera_prefix,
                })
        
        if not jobs:
            print("No episodes found to process.")
            return
        
        print(f"Found {len(jobs)} episodes to process across {len(compose_files)} instances")
        print(f"Processing with {workers} parallel workers...\n")
        
        # Process jobs in parallel
        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = {
                executor.submit(
                    self._process_single_recording,
                    job,
                    comparison_video,
                    output_dir
                ): job
                for job in jobs
            }
            
            completed = 0
            failed = 0
            
            for future in as_completed(futures):
                job = futures[future]
                try:
                    success = future.result()
                    completed += 1
                    status = "‚úÖ" if success else "‚ùå"
                    episode_name = job['episode_file'].stem
                    print(f"[{completed}/{len(jobs)}] {status} {job['bot']} instance {job['instance_id']}: {episode_name}")
                    if not success:
                        failed += 1
                except Exception as e:
                    failed += 1
                    print(f"[{completed+failed}/{len(jobs)}] ‚ùå Error: {e}")
        
        print(f"\n{'='*60}")
        print(f"Postprocessing Summary:")
        print(f"  Total episodes: {len(jobs)}")
        print(f"  Successful: {completed - failed}")
        print(f"  Failed: {failed}")
        print(f"{'='*60}")


def main():
    parser = argparse.ArgumentParser(
        description="Orchestrate multiple Docker Compose instances"
    )
    parser.add_argument(
        "command",
        choices=["start", "stop", "status", "logs", "recordings", "postprocess"],
        help="Command to execute",
    )
    parser.add_argument(
        "--compose-dir",
        default="compose_configs",
        help="Directory containing Docker Compose files",
    )
    parser.add_argument("--instance", help="Instance pattern for logs command")
    parser.add_argument(
        "--follow",
        "-f",
        action="store_true",
        help="Follow logs (only for single instance)",
    )
    parser.add_argument(
        "--tail",
        type=int,
        default=50,
        help="Tail last N lines when showing saved logs (default: 50)",
    )
    parser.add_argument(
        "--build",
        action="store_true",
        help="Build images before starting (default: pull from Docker Hub)",
    )
    parser.add_argument(
        "--logs-dir",
        type=str,
        default="logs",
        help="Directory for storing logs (default: logs)",
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Number of parallel workers for postprocess (default: 4)",
    )
    parser.add_argument(
        "--comparison-video",
        action="store_true",
        help="Generate side-by-side comparison videos for postprocess (slower)",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug output for postprocess command",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        help="Directory for processed video outputs (default: batch1/aligned)",
    )

    args = parser.parse_args()

    manager = InstanceManager(args.compose_dir, build_images=args.build, logs_dir=args.logs_dir)

    if args.command == "start":
        manager.start_all()
    elif args.command == "stop":
        manager.stop_all()
    elif args.command == "status":
        manager.status()
    elif args.command == "logs":
        manager.logs(args.instance, args.follow, args.tail)
    elif args.command == "recordings":
        manager.recordings()
    elif args.command == "postprocess":
        manager.postprocess_recordings(args.workers, args.comparison_video, args.debug, args.output_dir)


if __name__ == "__main__":
    main()
